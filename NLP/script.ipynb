{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sentiment:  # enum class\n",
    "    NEGATIVE = 'NEGATIVE'\n",
    "    NEUTRAL = 'NEUTRAL'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):  # 'self' is a necessary initialization, other two are optional\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else: \n",
    "            return Sentiment.POSITIVE\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia's trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character's voice on a strong subject and making it so that other peoples story may be heard through Mia's.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = './sklearn-master/data/sentiment/Books_small_10000.json'\n",
    "\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        # reviews.append((review['reviewText'], review['overall']))     ## without using class\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))    ## using class\n",
    "\n",
    "# reviews[5][1]    ## without using class\n",
    "# reviews[5][0]    ## without using class\n",
    "\n",
    "print(reviews[5].score)    ## using class\n",
    "print(reviews[5].text)    ## using class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6700\n",
      "3300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # trying to split the data in training and test set\n",
    "\n",
    "training, test = train_test_split(reviews, test_size = 0.33, random_state = 42)\n",
    "print(len(training))\n",
    "print(len(test))\n",
    "\n",
    "train_container = ReviewContainer(training)\n",
    "test_container = ReviewContainer(test)\n",
    "\n",
    "train_container.evenly_distribute()\n",
    "test_container.evenly_distribute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "# train_x = [x.text for x in training]\n",
    "# train_y = [x.sentiment for x in training]\n",
    "\n",
    "# test_x = [x.text for x in test]\n",
    "# test_y = [x.sentiment for x in test]\n",
    "\n",
    "train_x = train_container.get_text()\n",
    "train_y = train_container.get_sentiment()\n",
    "\n",
    "test_x = test_container.get_text()\n",
    "test_y = test_container.get_sentiment()\n",
    "\n",
    "print(test_y.count(Sentiment.POSITIVE))\n",
    "print(test_y.count(Sentiment.NEGATIVE))\n",
    "\n",
    "# print(train_x[0], train_y[0], '\\n')\n",
    "# print(test_x[0], test_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is a really good choice for anyone who wants to learn about the Louisiana Purchase, but is not interested in a heavily documented academic study. I prefer a documented book on any history topic, but I realize that is not what many prefer. Fleming is an excellent writer and while this book is not documented it is a reliable source on this momentous event in United States history. Highly recommended.\n",
      "  (0, 6444)\t0.11390450271244093\n",
      "  (0, 3769)\t0.10358985835820621\n",
      "  (0, 7469)\t0.14312928345225817\n",
      "  (0, 8359)\t0.156877491045465\n",
      "  (0, 2783)\t0.15139547296682715\n",
      "  (0, 5164)\t0.17390619455317205\n",
      "  (0, 7338)\t0.156877491045465\n",
      "  (0, 6524)\t0.17390619455317205\n",
      "  (0, 4277)\t0.03573968486953971\n",
      "  (0, 8693)\t0.08800670271859598\n",
      "  (0, 423)\t0.031069135183508054\n",
      "  (0, 8838)\t0.10820978004877607\n",
      "  (0, 2814)\t0.12440561647121268\n",
      "  (0, 416)\t0.05922282121984397\n",
      "  (0, 3116)\t0.17390619455317205\n",
      "  (0, 4891)\t0.07829496533593017\n",
      "  (0, 8679)\t0.061518235586859364\n",
      "  (0, 7925)\t0.04367989242155538\n",
      "  (0, 6408)\t0.13695518506965015\n",
      "  (0, 8084)\t0.1469163380575576\n",
      "  (0, 3798)\t0.21475382592701125\n",
      "  (0, 465)\t0.07984709569702683\n",
      "  (0, 5512)\t0.10752776359177812\n",
      "  (0, 6073)\t0.2938326761151152\n",
      "  (0, 7586)\t0.13984878753775798\n",
      "  (0, 169)\t0.16394504156526463\n",
      "  (0, 2360)\t0.4918351246957939\n",
      "  (0, 3710)\t0.1469163380575576\n",
      "  (0, 4034)\t0.08061534120594974\n",
      "  (0, 4189)\t0.1043020017307502\n",
      "  (0, 5408)\t0.13617709917536586\n",
      "  (0, 1168)\t0.08974481137959849\n",
      "  (0, 6279)\t0.12792120725251144\n",
      "  (0, 4780)\t0.16394504156526463\n",
      "  (0, 7929)\t0.029369283196353207\n",
      "  (0, 149)\t0.05968856457325546\n",
      "  (0, 4600)\t0.10907187643684407\n",
      "  (0, 8052)\t0.03192730322477719\n",
      "  (0, 8592)\t0.11613942695664366\n",
      "  (0, 8704)\t0.06640598242877736\n",
      "  (0, 469)\t0.1043020017307502\n",
      "  (0, 3177)\t0.04242588571508001\n",
      "  (0, 1420)\t0.1320252627219175\n",
      "  (0, 3432)\t0.062290044383031595\n",
      "  (0, 6411)\t0.06534753822682397\n",
      "  (0, 4264)\t0.23993580219861957\n",
      "  (0, 991)\t0.10829109792170687\n",
      "  (0, 7976)\t0.09880479967679244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# CountVectorizer weights each word equally - Point to be noted\n",
    "# So we can use something like Term Frequency Inverse Document Frequency Vectorizer to handle that issue\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x) # 1 STEP PROCESS\n",
    "\n",
    "test_x_vectors = vectorizer.transform(test_x) # The reasons we didn't fit is\n",
    "# because we didn't wanna fit a new model for testing, the model will be the training model\n",
    "\n",
    "# vectorizer.fit(train_x)\n",
    "# train_x_vectors = vectorizer.transform(train_x) # 2 STEP PROCESS\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm absolutely sick and tired of the man cheats, woman sucks it up, no grovel romances.  Caden cheated.  More than once.  He had ZERO grovel time.  None.  Zip, zilch, nada.  Oh, but we are told he is sorry.  And remorseful.  Ever is like 'dude, you cheated.. How many times?' And flounces off for a WEEK, to come back and say I forgive you, for me, because I can't find a better man who won't step out on me.  Give me a break!  Do I want reality in my romances, not necessarily- I don't need discourse on hygiene, taxes, and mortgage payments, but I expect to read about realistic response when you find out your TWIN and your HUSBAND consistently bumped uglies whilst you were in a coma.  Reality- if Caden and Ever had a chance, it would have been after she dumped him, got some therapy, and found herself some equal footing.  Not a week and oh, I forgive you.  Eden didn't deserve squat and she is the one who walks away a winner.  Uh, yeah, no.  I was wondering how Wilder would make this work and she didn't.  At all.  Forgiveness in the eye of the beholder, love knowing no bounds, whatever, is fine and dandy.  This was a travesty.\n",
      "['NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel = 'linear') # setting the classifier as SVM\n",
    "clf_svm.fit(train_x_vectors, train_y) # training the model\n",
    "\n",
    "print(test_x[0])\n",
    "\n",
    "print(clf_svm.predict(test_x_vectors[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# clf_gnb = GaussianNB()\n",
    "# clf_gnb.fit(train_x_vectors.todense(), train_y)\n",
    "\n",
    "# clf_gnb.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_log.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM is: 80.77%\n",
      "Accuracy for Decision Tree is: 62.74%\n",
      "Accuracy for Logistic Regression is: 80.53%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for SVM is: ' + str(round(float(clf_svm.score(test_x_vectors, test_y))*100, 2)) + '%')\n",
    "print('Accuracy for Decision Tree is: ' + str(round(float(clf_dec.score(test_x_vectors, test_y))*100, 2)) + '%')\n",
    "print('Accuracy for Logistic Regression is: ' + str(round(float(clf_log.score(test_x_vectors, test_y))*100, 2)) + '%')\n",
    "\n",
    "# Case 1\n",
    "# Accuracy for SVM is: 82.42%\n",
    "# Accuracy for Decision Tree is: 76.36%\n",
    "# Accuracy for Logistic Regression is: 83.03%\n",
    "\n",
    "# Case 2\n",
    "# Accuracy for SVM is: 79.81%\n",
    "# Accuracy for Decision Tree is: 64.42%\n",
    "# Accuracy for Logistic Regression is: 81.49%\n",
    "\n",
    "# Case 3\n",
    "# Accuracy for SVM is: 80.77%\n",
    "# Accuracy for Decision Tree is: 62.74%\n",
    "# Accuracy for Logistic Regression is: 80.53%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80582524 0.80952381]\n",
      "[0.61728395 0.63700234]\n",
      "[0.80291971 0.80760095]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(test_y, clf_svm.predict(test_x_vectors), average = None, labels = [Sentiment.POSITIVE, Sentiment.NEGATIVE]))\n",
    "print(f1_score(test_y, clf_dec.predict(test_x_vectors), average = None, labels = [Sentiment.POSITIVE, Sentiment.NEGATIVE]))\n",
    "print(f1_score(test_y, clf_log.predict(test_x_vectors), average = None, labels = [Sentiment.POSITIVE, Sentiment.NEGATIVE]))\n",
    "\n",
    "# Case 1\n",
    "# [0.91319444 0.21052632 0.22222222]\n",
    "# [0.87170475 0.1        0.06451613]\n",
    "# [0.91370558 0.12244898 0.1       ]\n",
    "\n",
    "# Case 2\n",
    "# [0.8028169  0.79310345]\n",
    "# [0.65258216 0.63546798]\n",
    "# [0.82051282 0.808933  ]  ## Didn't include NEUTRAL\n",
    "\n",
    "# Case 3\n",
    "# [0.80582524 0.80952381]\n",
    "# [0.61728395 0.63700234]\n",
    "# [0.80291971 0.80760095]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At this point we can see that all our models are performing very well when it comes to predicting the POSITIVE part of the rating but very bad when it comes to predicting the NEUTRAL or the NEGATIVE. So now, we will be improving the NEUTRAL/NEGATIVE part of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "print(train_y.count(Sentiment.POSITIVE))\n",
    "print(train_y.count(Sentiment.NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It is quite evident that the number of positive versus the number of negative sentiments is very heavily biased towards the positive side, hence, our model will also likely be biased, which shows in the F1 score of the testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we will go up again at the beginning and replace the current JSON file with the new JSON file containing not just 1000, but around 10000 data points so our model can be trained better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['This book is not good', 'horrible book', 'interesting book']\n",
    "new_test = vectorizer.transform(test_set)\n",
    "\n",
    "clf_svm.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: (1, 4, 8, 16, 32), &#x27;kernel&#x27;: (&#x27;linear&#x27;, &#x27;rbf&#x27;)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: (1, 4, 8, 16, 32), &#x27;kernel&#x27;: (&#x27;linear&#x27;, &#x27;rbf&#x27;)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': (1, 4, 8, 16, 32), 'kernel': ('linear', 'rbf')})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# to find out the best parameters to be used within a classifier parameter list\n",
    "\n",
    "parameters = {'kernel': ('linear', 'rbf'), 'C': (1,4,8,16,32)}\n",
    "\n",
    "svc = svm.SVC()\n",
    "tuned_clf_svm = GridSearchCV(svc, parameters, cv = 5) # 'cv' stands for cross validation, \n",
    "# or number of iterations to check the optimal parameter values\n",
    "\n",
    "tuned_clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "dec = DecisionTreeClassifier()\n",
    "tuned_clf_dec = GridSearchCV(svc, parameters, cv = 5)\n",
    "\n",
    "tuned_clf_dec.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM is: 80.77%\n",
      "Accuracy for Decision Tree is: 80.77%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for SVM is: ' + str(round(float(tuned_clf_svm.score(test_x_vectors, test_y))*100, 2)) + '%')\n",
    "print('Accuracy for Decision Tree is: ' + str(round(float(tuned_clf_dec.score(test_x_vectors, test_y))*100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # for saving the models\n",
    "\n",
    "with open('./models/sentiment_classifier_dectree.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_clf_dec, f)\n",
    "\n",
    "with open('./models/sentiment_classifier_svm.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_clf_svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/sentiment_classifier_dectree.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm absolutely sick and tired of the man cheats, w\n",
      "['NEGATIVE']\n",
      "Although the Navajo element was interesting as bac\n",
      "['NEGATIVE']\n",
      "Enjoyed the book at lot. It has a very good plot. \n",
      "['POSITIVE']\n",
      "did not serve my need.\n",
      "['NEGATIVE']\n",
      "Well, the story did have potential but oh boy, I'v\n",
      "['NEGATIVE']\n",
      "All my annoyance melted. \"You dumb-a@#,\" I crooned\n",
      "['POSITIVE']\n",
      "Confusing, too many tangents, easily forgotten\n",
      "['NEGATIVE']\n",
      "I love HM Ward and I love the Fero boys. But becau\n",
      "['POSITIVE']\n",
      "Intercepting Love- LP Dover ( 4 Stars)This book wa\n",
      "['NEGATIVE']\n",
      "My husband and I got new computers each and he had\n",
      "['NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(test_x[i]) if len(test_x[i]) < 50 else print(test_x[i][:50])\n",
    "    print(loaded_clf.predict(test_x_vectors[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
